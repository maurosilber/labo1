{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Herramientas para laboratorio 1","text":"In\u00a0[1]: Copied! <pre>from labo1 import to_significant_figures\n</pre> from labo1 import to_significant_figures <p>Redondear un n\u00famero a <code>n=2</code> cifras significativas:</p> In\u00a0[2]: Copied! <pre>to_significant_figures(0.00123456789, n=2)\n</pre> to_significant_figures(0.00123456789, n=2) Out[2]: <pre>'0.0012'</pre> <p>Redondear una medici\u00f3n y su incerteza a <code>n=2</code> cifras significativas:</p> In\u00a0[3]: Copied! <pre>to_significant_figures(123.456789, 0.00123456789, n=2)\n</pre> to_significant_figures(123.456789, 0.00123456789, n=2) Out[3]: <pre>('123.4568', '0.0012')</pre> In\u00a0[4]: Copied! <pre>import numpy as np\nfrom labo1 import curve_fit\n\nx = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\ny = np.array([2.4, 5.3, 6.6, 9.6, 11.0])\n\n\ndef lineal(x, a, b):\n    return a * x + b\n\n\nresult = curve_fit(lineal, x, y)\nresult.plot()\nresult\n</pre> import numpy as np from labo1 import curve_fit  x = np.array([1.0, 2.0, 3.0, 4.0, 5.0]) y = np.array([2.4, 5.3, 6.6, 9.6, 11.0])   def lineal(x, a, b):     return a * x + b   result = curve_fit(lineal, x, y) result.plot() result Out[4]: <pre>Result(a=2.15 \u00b1 0.16, b=0.53 \u00b1 0.52)</pre> <p>En las siguientes secciones, hay m\u00e1s detalles sobre las funcionalidades de <code>curve_fit</code>:</p> <ul> <li>B\u00e1sico: como acceder a los par\u00e1metros, errores y graficar los residuos.</li> <li>Ponderado: como ponderar los errores en un ajuste.</li> <li>No lineal: como pasar par\u00e1metros iniciales a un ajuste.</li> <li>Gr\u00e1ficos: como realizar gr\u00e1ficos m\u00e1s complejos.</li> </ul>"},{"location":"#herramientas-para-laboratorio-1","title":"Herramientas para laboratorio 1\u00b6","text":"<p>Este es un paquete de Python para facilitar el an\u00e1lisis de datos en Laboratorio 1.</p> <p>Se puede instalar con <code>pip</code>:</p> <pre><code>pip install labo1\n</code></pre> <p>En Google Colab, a\u00f1adir un signo de exclamaci\u00f3n al principio del comando:</p> <pre><code>!pip install labo1\n</code></pre>"},{"location":"#cifras-significativas","title":"Cifras significativas\u00b6","text":"<p>Para redondear n\u00fameros y mediciones a una cantidad de cifras significativas, podemos usar:</p>"},{"location":"#ajustes-por-cuadrados-minimos","title":"Ajustes por cuadrados m\u00ednimos\u00b6","text":"<p>Para hacer ajustes, podemos usar <code>curve_fit</code>.</p> <p>Para definir la funci\u00f3n a ajustar, tenemos que poner primero la variable independiente <code>x</code> y luego los par\u00e1metros. En el ejemplo debajo, la funci\u00f3n a ajustar es</p> <p>$$ f(x) = a x + b $$</p> <p><code>curve_fit</code> nos devuelve un objeto con el resultado del ajuste con el cu\u00e1l podemos realizar r\u00e1pidamente un gr\u00e1fico:</p>"},{"location":"API/","title":"API","text":""},{"location":"API/#labo1.to_significant_figures","title":"labo1.to_significant_figures","text":"<pre><code>to_significant_figures(x: float, dx: float | None = None, /, n: int = 2)\n</code></pre> <p>Rounds to <code>n</code> significant figures based on the uncertainty <code>dx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of significant figures.</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; to_significant_figures(0.1234, n=2)\n'0.12'\n&gt;&gt;&gt; to_significant_figures(12.34, n=2)\n'12'\n&gt;&gt;&gt; to_significant_figures(1234, n=2)\n'1200'\n&gt;&gt;&gt; to_significant_figures(12.34, 5.678, n=2)\n('12.3', '5.7')\n</code></pre> Source code in <code>src/labo1/round.py</code> <pre><code>def to_significant_figures(\n    x: float,\n    dx: float | None = None,\n    /,\n    n: int = 2,\n):\n    \"\"\"Rounds to `n` significant figures based on the uncertainty `dx`.\n\n    Parameters:\n        n: Number of significant figures.\n\n    Examples:\n        &gt;&gt;&gt; to_significant_figures(0.1234, n=2)\n        '0.12'\n        &gt;&gt;&gt; to_significant_figures(12.34, n=2)\n        '12'\n        &gt;&gt;&gt; to_significant_figures(1234, n=2)\n        '1200'\n        &gt;&gt;&gt; to_significant_figures(12.34, 5.678, n=2)\n        ('12.3', '5.7')\n    \"\"\"\n    if dx is None:\n        return to_significant_figures(x, x, n=n)[0]\n\n    if dx == 0:\n        decimals = n - 1\n    else:\n        decimals = n - np.ceil(np.log10(dx)).astype(int)\n\n    if decimals &lt; 0:\n        x = round(x, decimals)\n        dx = round(dx, decimals)\n        decimals = 0\n\n    return f\"{x:.{decimals}f}\", f\"{dx:.{decimals}f}\"\n</code></pre>"},{"location":"API/#labo1.curve_fit","title":"labo1.curve_fit","text":"<pre><code>curve_fit(\n    func: Callable[..., np.ndarray],\n    /,\n    x: ArrayLike,\n    y: ArrayLike,\n    y_err: ArrayLike | None = None,\n    *,\n    initial_params: Sequence[float] | Mapping[str, float] | None = None,\n    rescale_errors: bool = True,\n    **kwargs,\n)\n</code></pre> <p>Use non-linear least squares to fit a function to data.</p> <p>Returns a <code>Result</code> object with the parameters, errors, and methods to quickly plot the fit.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., ndarray]</code> <p>The function to fit. Its signature must start with the independent variable <code>x</code></p> required <code>followed</code> <code>by its N parameters to fit</code> <p><code>f(x, p_0, p_1, ...)</code>.</p> required <code>y_err</code> <code>ArrayLike | None</code> <p>Errors or uncertainties for <code>y</code>.</p> <code>None</code> <code>initial_params</code> <code>Sequence[float] | Mapping[str, float] | None</code> <p>Initial guess for the parameters.</p> <code>None</code> <code>rescale_errors</code> <code>bool</code> <p>Whether to estimate a scale factor for the errors based on the residuals.</p> <code>True</code> <code>**kwargs</code> <p>Passed to scipy.optimize.curve_fit.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def f(x, a, b):\n...     return a * x + b\n...\n&gt;&gt;&gt; x = np.array([0.0, 1.0, 2.0])\n&gt;&gt;&gt; y = np.array([0.0, 0.9, 2.1])\n&gt;&gt;&gt; curve_fit(f, x, y)\nResult(a=1.050 \u00b1 0.087, b=-0.05 \u00b1 0.11)\n</code></pre> Source code in <code>src/labo1/fit.py</code> <pre><code>def curve_fit(\n    func: Callable[..., np.ndarray],\n    /,\n    x: ArrayLike,\n    y: ArrayLike,\n    y_err: ArrayLike | None = None,\n    *,\n    initial_params: Sequence[float] | Mapping[str, float] | None = None,\n    rescale_errors: bool = True,\n    **kwargs,\n):\n    \"\"\"Use non-linear least squares to fit a function to data.\n\n    Returns a `Result` object with the parameters, errors,\n    and methods to quickly plot the fit.\n\n    Parameters:\n        func: The function to fit. Its signature must start with the independent variable `x`\n        followed by its N parameters to fit: `f(x, p_0, p_1, ...)`.\n        y_err: Errors or uncertainties for `y`.\n        initial_params: Initial guess for the parameters.\n        A sequence of length N or a mapping of names to values,\n        where omitted values default to 1.\n        rescale_errors: Whether to estimate a scale factor for the errors based on the residuals.\n        **kwargs: Passed to scipy.optimize.curve_fit.\n\n    Examples:\n        &gt;&gt;&gt; def f(x, a, b):\n        ...     return a * x + b\n        ...\n        &gt;&gt;&gt; x = np.array([0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y = np.array([0.0, 0.9, 2.1])\n        &gt;&gt;&gt; curve_fit(f, x, y)\n        Result(a=1.050 \u00b1 0.087, b=-0.05 \u00b1 0.11)\n    \"\"\"\n    # accept ArrayLike\n    x = np.asarray(x)\n    y = np.asarray(y)\n    if y_err is not None:\n        y_err = np.asarray(y_err)\n\n    if isinstance(initial_params, Mapping):\n        names = _get_parameter_names(func)\n        unused_params = initial_params.keys() - names\n        if len(unused_params) &gt; 0:\n            warn(f\"unused parameters: {unused_params}\")\n\n        initial_params = [initial_params.get(name, 1) for name in names]  # type: ignore\n\n    p, cov = scipy.optimize.curve_fit(\n        func,\n        x,\n        y,\n        p0=initial_params,\n        sigma=y_err,\n        absolute_sigma=not rescale_errors,\n        **kwargs,\n    )\n    return Result(func, p, cov, x=x, y=y, y_err=y_err)\n</code></pre>"},{"location":"API/#labo1.fit.Result","title":"labo1.fit.Result  <code>dataclass</code>","text":"Source code in <code>src/labo1/fit.py</code> <pre><code>@dataclass(frozen=True)\nclass Result:\n    func: Callable[..., np.ndarray]\n    params: np.ndarray\n    \"Optimal parameters found by least squares.\"\n    covariance: np.ndarray\n    \"Covariance matrix of the parameters.\"\n    x: np.ndarray\n    y: np.ndarray\n    y_err: np.ndarray\n\n    @property\n    def names(self) -&gt; Sequence[str]:\n        \"\"\"Names of the parameters.\n\n        Extracted from the function signature.\"\"\"\n        return _get_parameter_names(self.func)\n\n    @property\n    def errors(self) -&gt; np.ndarray:\n        \"\"\"Standard deviation for the parameters.\n\n        The square-root of the diagonal of the covariance matrix.\"\"\"\n        return np.sqrt(np.diag(self.covariance))\n\n    def __getitem__(self, item: int | str) -&gt; tuple[float, float]:\n        if isinstance(item, str):\n            item = self.names.index(item)\n        return self.params[item], self.errors[item]\n\n    def plot(\n        self,\n        *,\n        x_eval: int | np.ndarray | None = None,\n        x_err: np.ndarray | None = None,\n        label: str | None = None,\n        fig: Figure | SubFigure | None = None,\n        axes: Axes | None = None,\n    ) -&gt; tuple[Figure | SubFigure, Axes]:\n        \"\"\"Errorbar plot of the data and line plot of the function.\n\n        Parameters:\n            x_eval: Evaluation points for the line plot of the function.\n            For an `int`, it generates equispaced points between the minimum and maximum of `x`.\n            By default, `x_eval = x`.\n            x_err: Error bars for `x`.\n            label: Name of the line plot for the legend.\n            axes: Axes on which to plot.\n            By default, creates a new axes on `fig`.\n            fig: Figure on which to create the `axes`.\n            By default, creates a new figure.\n\n        Returns:\n            The axes on which it plotted and its corresponding figure.\n        \"\"\"\n        return with_errorbars(\n            self.func,\n            self.params,\n            self.x,\n            self.y,\n            y_err=self.y_err,\n            x_err=x_err,\n            x_eval=x_eval,\n            label=label,\n            fig=fig,\n            axes=axes,\n        )\n\n    def plot_with_residuals(\n        self,\n        *,\n        x_eval: np.ndarray | None = None,\n        x_err: np.ndarray | None = None,\n        label: str | None = None,\n        fig: Figure | SubFigure | None = None,\n        axes: Sequence[Axes] | None = None,\n    ) -&gt; tuple[Figure | SubFigure, Sequence[Axes]]:\n        \"\"\"Errorbar plot of the data and residuals, and line plot of the function.\n\n        Parameters:\n            x_eval: Evaluation points for the line plot of the function.\n            For an `int`, it generates equispaced points between the minimum and maximum of `x`.\n            By default, `x_eval = x`.\n            x_err: Error bars for `x`.\n            label: Name of the line plot for the legend.\n            axes: Axes on which to plot.\n            By default, creates a new axes on `fig`.\n            fig: Figure on which to create the `axes`.\n            By default, creates a new figure.\n\n        Returns:\n            The axes on which it plotted and its corresponding figure.\n        \"\"\"\n        return with_residuals(\n            self.func,\n            self.params,\n            self.x,\n            self.y,\n            y_err=self.y_err,\n            x_err=x_err,\n            x_eval=x_eval,\n            label=label,\n            fig=fig,\n            axes=axes,\n        )\n\n    @property\n    def residuals(self):\n        \"\"\"The difference between the measured and predicted `y`.\n\n        $$ r_i = y_i - f(x_i) $$\n        \"\"\"\n        return self.y - self.func(self.x, *self.params)\n\n    @property\n    def standardized_residuals(self):\n        \"\"\"Residuals divided by their corresponding error.\"\"\"\n        return self.residuals / self.y_err\n\n    @property\n    def chi2(self):\n        r\"\"\"Sum of the standardized squared residuals.\n\n        $$ \\chi^2 = \\sum_i (\\frac{r_i}{y_{err}_i})^2 $$\n        \"\"\"\n        return np.sum(self.standardized_residuals**2)\n\n    @property\n    def reduced_chi2(self):\n        \"\"\"\u03c7\u00b2 divided by the degree of freedom.\n\n        The degree of freedom is the number of measuments minus the number of fitter parameters.\n        \"\"\"\n        return self.chi2 / (np.size(self.y) - np.size(self.params))\n\n    def __str__(self):\n        values = {\n            name: to_significant_figures(x, dx)\n            for name, x, dx in zip(self.names, self.params, self.errors)\n        }\n        values = [f\"{name}={x} \u00b1 {dx}\" for name, (x, dx) in values.items()]\n        return \", \".join(values)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self})\"\n</code></pre>"},{"location":"API/#labo1.fit.Result.chi2","title":"chi2  <code>property</code>","text":"<pre><code>chi2\n</code></pre> <p>Sum of the standardized squared residuals.</p> \\[ \\chi^2 = \\sum_i (\\frac{r_i}{y_{err}_i})^2 \\]"},{"location":"API/#labo1.fit.Result.covariance","title":"covariance  <code>instance-attribute</code>","text":"<pre><code>covariance: ndarray\n</code></pre> <p>Covariance matrix of the parameters.</p>"},{"location":"API/#labo1.fit.Result.errors","title":"errors  <code>property</code>","text":"<pre><code>errors: ndarray\n</code></pre> <p>Standard deviation for the parameters.</p> <p>The square-root of the diagonal of the covariance matrix.</p>"},{"location":"API/#labo1.fit.Result.names","title":"names  <code>property</code>","text":"<pre><code>names: Sequence[str]\n</code></pre> <p>Names of the parameters.</p> <p>Extracted from the function signature.</p>"},{"location":"API/#labo1.fit.Result.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: ndarray\n</code></pre> <p>Optimal parameters found by least squares.</p>"},{"location":"API/#labo1.fit.Result.reduced_chi2","title":"reduced_chi2  <code>property</code>","text":"<pre><code>reduced_chi2\n</code></pre> <p>\u03c7\u00b2 divided by the degree of freedom.</p> <p>The degree of freedom is the number of measuments minus the number of fitter parameters.</p>"},{"location":"API/#labo1.fit.Result.residuals","title":"residuals  <code>property</code>","text":"<pre><code>residuals\n</code></pre> <p>The difference between the measured and predicted <code>y</code>.</p> \\[ r_i = y_i - f(x_i) \\]"},{"location":"API/#labo1.fit.Result.standardized_residuals","title":"standardized_residuals  <code>property</code>","text":"<pre><code>standardized_residuals\n</code></pre> <p>Residuals divided by their corresponding error.</p>"},{"location":"API/#labo1.fit.Result.plot","title":"plot","text":"<pre><code>plot(\n    *,\n    x_eval: int | np.ndarray | None = None,\n    x_err: np.ndarray | None = None,\n    label: str | None = None,\n    fig: Figure | SubFigure | None = None,\n    axes: Axes | None = None\n) -&gt; tuple[Figure | SubFigure, Axes]\n</code></pre> <p>Errorbar plot of the data and line plot of the function.</p> <p>Parameters:</p> Name Type Description Default <code>x_eval</code> <code>int | ndarray | None</code> <p>Evaluation points for the line plot of the function.</p> <code>None</code> <code>x_err</code> <code>ndarray | None</code> <p>Error bars for <code>x</code>.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Name of the line plot for the legend.</p> <code>None</code> <code>axes</code> <code>Axes | None</code> <p>Axes on which to plot.</p> <code>None</code> <code>fig</code> <code>Figure | SubFigure | None</code> <p>Figure on which to create the <code>axes</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Figure | SubFigure, Axes]</code> <p>The axes on which it plotted and its corresponding figure.</p> Source code in <code>src/labo1/fit.py</code> <pre><code>def plot(\n    self,\n    *,\n    x_eval: int | np.ndarray | None = None,\n    x_err: np.ndarray | None = None,\n    label: str | None = None,\n    fig: Figure | SubFigure | None = None,\n    axes: Axes | None = None,\n) -&gt; tuple[Figure | SubFigure, Axes]:\n    \"\"\"Errorbar plot of the data and line plot of the function.\n\n    Parameters:\n        x_eval: Evaluation points for the line plot of the function.\n        For an `int`, it generates equispaced points between the minimum and maximum of `x`.\n        By default, `x_eval = x`.\n        x_err: Error bars for `x`.\n        label: Name of the line plot for the legend.\n        axes: Axes on which to plot.\n        By default, creates a new axes on `fig`.\n        fig: Figure on which to create the `axes`.\n        By default, creates a new figure.\n\n    Returns:\n        The axes on which it plotted and its corresponding figure.\n    \"\"\"\n    return with_errorbars(\n        self.func,\n        self.params,\n        self.x,\n        self.y,\n        y_err=self.y_err,\n        x_err=x_err,\n        x_eval=x_eval,\n        label=label,\n        fig=fig,\n        axes=axes,\n    )\n</code></pre>"},{"location":"API/#labo1.fit.Result.plot_with_residuals","title":"plot_with_residuals","text":"<pre><code>plot_with_residuals(\n    *,\n    x_eval: np.ndarray | None = None,\n    x_err: np.ndarray | None = None,\n    label: str | None = None,\n    fig: Figure | SubFigure | None = None,\n    axes: Sequence[Axes] | None = None\n) -&gt; tuple[Figure | SubFigure, Sequence[Axes]]\n</code></pre> <p>Errorbar plot of the data and residuals, and line plot of the function.</p> <p>Parameters:</p> Name Type Description Default <code>x_eval</code> <code>ndarray | None</code> <p>Evaluation points for the line plot of the function.</p> <code>None</code> <code>x_err</code> <code>ndarray | None</code> <p>Error bars for <code>x</code>.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Name of the line plot for the legend.</p> <code>None</code> <code>axes</code> <code>Sequence[Axes] | None</code> <p>Axes on which to plot.</p> <code>None</code> <code>fig</code> <code>Figure | SubFigure | None</code> <p>Figure on which to create the <code>axes</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Figure | SubFigure, Sequence[Axes]]</code> <p>The axes on which it plotted and its corresponding figure.</p> Source code in <code>src/labo1/fit.py</code> <pre><code>def plot_with_residuals(\n    self,\n    *,\n    x_eval: np.ndarray | None = None,\n    x_err: np.ndarray | None = None,\n    label: str | None = None,\n    fig: Figure | SubFigure | None = None,\n    axes: Sequence[Axes] | None = None,\n) -&gt; tuple[Figure | SubFigure, Sequence[Axes]]:\n    \"\"\"Errorbar plot of the data and residuals, and line plot of the function.\n\n    Parameters:\n        x_eval: Evaluation points for the line plot of the function.\n        For an `int`, it generates equispaced points between the minimum and maximum of `x`.\n        By default, `x_eval = x`.\n        x_err: Error bars for `x`.\n        label: Name of the line plot for the legend.\n        axes: Axes on which to plot.\n        By default, creates a new axes on `fig`.\n        fig: Figure on which to create the `axes`.\n        By default, creates a new figure.\n\n    Returns:\n        The axes on which it plotted and its corresponding figure.\n    \"\"\"\n    return with_residuals(\n        self.func,\n        self.params,\n        self.x,\n        self.y,\n        y_err=self.y_err,\n        x_err=x_err,\n        x_eval=x_eval,\n        label=label,\n        fig=fig,\n        axes=axes,\n    )\n</code></pre>"},{"location":"ajustes/basico/","title":"B\u00e1sico","text":"<p>Para realizar un ajuste, definimos la funci\u00f3n a ajustar y se la pasamos a <code>curve_fit</code> junto con los datos:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom labo1 import curve_fit\n\n\ndef lineal(x, a, b):\n    return a * x + b\n\n\nresult = curve_fit(\n    lineal,\n    x=np.array([1.0, 2.0, 3.0, 4.0, 5.0]),\n    y=np.array([2.9, 5.0, 6.9, 9.0, 11.0]),\n)\n</pre> import numpy as np from labo1 import curve_fit   def lineal(x, a, b):     return a * x + b   result = curve_fit(     lineal,     x=np.array([1.0, 2.0, 3.0, 4.0, 5.0]),     y=np.array([2.9, 5.0, 6.9, 9.0, 11.0]), ) <p><code>result</code> es un objeto que encapsula el resultado del ajuste, mostr\u00e1ndonos los par\u00e1metros obtenidos con su error:</p> In\u00a0[2]: Copied! <pre>result\n</pre> result Out[2]: <pre>Result(a=2.020 \u00b1 0.016, b=0.900 \u00b1 0.054)</pre> <p>Podemos acceder a ellos con:</p> In\u00a0[3]: Copied! <pre>result.params\n</pre> result.params Out[3]: <pre>array([2.02, 0.9 ])</pre> <p>y a sus incertezas con:</p> In\u00a0[4]: Copied! <pre>result.errors\n</pre> result.errors Out[4]: <pre>array([0.01632993, 0.05416026])</pre> <p>El orden es el mismo que en la definici\u00f3n de la funci\u00f3n:</p> In\u00a0[5]: Copied! <pre>lineal\n</pre> lineal Out[5]: <pre>&lt;function __main__.lineal(x, a, b)&gt;</pre> <p>Tambi\u00e9n es posible acceder al valor y su incerteza por nombre:</p> In\u00a0[6]: Copied! <pre>result[\"a\"]\n</pre> result[\"a\"] Out[6]: <pre>(2.020000000002225, 0.016329932000876773)</pre> <p>Se puede realizar un gr\u00e1fico r\u00e1pidamente con:</p> In\u00a0[7]: Copied! <pre>result.plot()\n</pre> result.plot() Out[7]: <pre>(&lt;Figure size 600x300 with 1 Axes&gt;, &lt;Axes: &gt;)</pre> <p>El m\u00e9todo <code>.plot</code> nos devuelve la figura y los ejes de <code>matplotlib</code>, a los cuales podemos agregarle nombres con:</p> In\u00a0[8]: Copied! <pre>fig, ax = result.plot()\nax.set(xlabel=\"eje x\", ylabel=\"eje y\")\n</pre> fig, ax = result.plot() ax.set(xlabel=\"eje x\", ylabel=\"eje y\") Out[8]: <pre>[Text(0.5, 0, 'eje x'), Text(0, 0.5, 'eje y')]</pre> <p>Para realizar un gr\u00e1fico con residuos llamamos al m\u00e9todo <code>.plot_with_residuals()</code>. A diferencia de <code>.plot</code>, este nos devuelve dos ejes:</p> In\u00a0[9]: Copied! <pre>fig, axes = result.plot_with_residuals()\n\naxes[0].set(ylabel=\"y [unidad]\")\naxes[1].set(ylabel=\"Residuos\", xlabel=\"x [unidad]\")\nfig.align_labels()\n</pre> fig, axes = result.plot_with_residuals()  axes[0].set(ylabel=\"y [unidad]\") axes[1].set(ylabel=\"Residuos\", xlabel=\"x [unidad]\") fig.align_labels()"},{"location":"ajustes/basico/#basico","title":"B\u00e1sico\u00b6","text":""},{"location":"ajustes/basico/#acceder-a-los-parametros","title":"Acceder a los par\u00e1metros\u00b6","text":""},{"location":"ajustes/basico/#graficar-el-ajuste","title":"Graficar el ajuste\u00b6","text":""},{"location":"ajustes/basico/#graficar-el-ajuste-con-residuos","title":"Graficar el ajuste con residuos\u00b6","text":""},{"location":"ajustes/graficos/","title":"Graficando el ajuste","text":"<p>El objeto <code>Result</code> tiene dos m\u00e9todos para realizar gr\u00e1ficos:</p> <ul> <li><code>.plot()</code>, que realiza un gr\u00e1fico de las mediciones y el ajuste,</li> <li><code>.plot_with_residuals()</code>, que a\u00f1ade un gr\u00e5fico de los residuos.</li> </ul> <p>En ambos casos, se incluye barras de error en <code>y</code> si fueron parte del ajuste.</p> <p>Adem\u00e1s, aceptan los siguientes par\u00e1metros:</p> <ul> <li><code>x_err</code>, para gr\u00e1ficar barras de error en <code>x</code>,</li> <li><code>x_eval</code>, para evaluar la funci\u00f3n ajustada en m\u00e1s puntos,</li> <li><code>label</code>, para darle un nombre a la curva del ajuste en la leyenda,</li> <li><code>fig</code> y <code>axes</code>, que permiten gr\u00e1ficar sobre un g\u00aeafico ya existente.</li> </ul> <p>Generemos y ajustemos unos datos para ver estas distintas opciones:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom labo1 import curve_fit\n\n\ndef func(x, A, w):\n    return A * np.cos(w * x)\n\n\nx = np.linspace(0, 10, 10)\ny = func(x, A=10, w=1)\n\ny = np.random.default_rng(0).normal(y)\n\nresult = curve_fit(func, x, y)\nresult\n</pre> import numpy as np from labo1 import curve_fit   def func(x, A, w):     return A * np.cos(w * x)   x = np.linspace(0, 10, 10) y = func(x, A=10, w=1)  y = np.random.default_rng(0).normal(y)  result = curve_fit(func, x, y) result Out[1]: <pre>Result(A=10.52 \u00b1 0.24, w=0.9911 \u00b1 0.0044)</pre> In\u00a0[2]: Copied! <pre>result.plot()\n</pre> result.plot() Out[2]: <pre>(&lt;Figure size 600x300 with 1 Axes&gt;, &lt;Axes: &gt;)</pre> In\u00a0[3]: Copied! <pre>result.plot(x_eval=100)\n</pre> result.plot(x_eval=100) Out[3]: <pre>(&lt;Figure size 600x300 with 1 Axes&gt;, &lt;Axes: &gt;)</pre> <p>O un <code>array</code> con los valores a evaluar:</p> In\u00a0[4]: Copied! <pre>result.plot(x_eval=np.linspace(-5, 15, 100))\n</pre> result.plot(x_eval=np.linspace(-5, 15, 100)) Out[4]: <pre>(&lt;Figure size 600x300 with 1 Axes&gt;, &lt;Axes: &gt;)</pre> In\u00a0[5]: Copied! <pre>fig, axes = result.plot(x_eval=20, label=\"10\")\nresult.plot(axes=axes, x_eval=100, label=\"100\")\naxes.legend(title=\"Ajuste\")\n</pre> fig, axes = result.plot(x_eval=20, label=\"10\") result.plot(axes=axes, x_eval=100, label=\"100\") axes.legend(title=\"Ajuste\") Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x135f78740&gt;</pre>"},{"location":"ajustes/graficos/#graficando-el-ajuste","title":"Graficando el ajuste\u00b6","text":""},{"location":"ajustes/graficos/#grafico-por-defecto","title":"Gr\u00e1fico por defecto\u00b6","text":"<p>Este grafica la funci\u00f3n para los mismos <code>x</code> que las mediciones. En este caso, vemos que no es suficiente para obtener una curva suave:</p>"},{"location":"ajustes/graficos/#evaluando-en-mas-puntos","title":"Evaluando en m\u00e1s puntos\u00b6","text":"<p>Se puede pedir que eval\u00fae en m\u00e1s puntos pasando un n\u00famero a <code>x_eval</code>:</p>"},{"location":"ajustes/graficos/#multiples-graficos","title":"M\u00faltiples gr\u00e1ficos\u00b6","text":"<p>El m\u00e9todo <code>.plot</code> nos devuelve una figura y los ejes. Podemos realizar otro gr\u00e1fico sobre los mismos ejes si se los pasamos al par\u00e1metro <code>axes</code> de la funci\u00f3n. En este caso, es \u00fatil ponerle un nombre a cada curva con el par\u00e1metro <code>label</code> y generar una leyenda con <code>axes.legend</code>:</p>"},{"location":"ajustes/no-lineal/","title":"Funciones no lineales","text":"<p>Para ajustar funciones no lineales, suele ser necesario pasarle par\u00e1metros iniciales para que encuentre la soluci\u00f3n correcta.</p> <p>Por ejemplo, si generamos datos que sigan una funci\u00f3n:</p> <p>$$ f(x) = A \\cos(\\omega x) $$</p> <p>y tratamos de ajustarlos por eseta funci\u00f3n, vemos que no ebtiene los par\u00e1metros correctos:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom labo1 import curve_fit\n\n\ndef func(x, A, w):\n    return A * np.cos(w * x)\n\n\nx = np.linspace(0, 10, 100)\ny = func(x, A=10, w=3)\n\ny = np.random.default_rng(0).normal(y)\n\nr = curve_fit(func, x, y)\nr.plot()\nr\n</pre> import numpy as np from labo1 import curve_fit   def func(x, A, w):     return A * np.cos(w * x)   x = np.linspace(0, 10, 100) y = func(x, A=10, w=3)  y = np.random.default_rng(0).normal(y)  r = curve_fit(func, x, y) r.plot() r Out[1]: <pre>Result(A=0.8 \u00b1 1.0, w=0.95 \u00b1 0.24)</pre> <p>Si le pasamos param\u00e9tros iniciales cercanos a los reales, vemos que obtiene una mejor aproximaci\u00f3n:</p> In\u00a0[2]: Copied! <pre>r = curve_fit(func, x, y, initial_params=(10, 3))\nr.plot()\nr\n</pre> r = curve_fit(func, x, y, initial_params=(10, 3)) r.plot() r Out[2]: <pre>Result(A=10.17 \u00b1 0.13, w=2.9951 \u00b1 0.0022)</pre> <p>Al igual que los par\u00e1metros finales, los par\u00e1metros iniciales tienen que estar en el mismo orden que en el que se defini\u00f3 en la funci\u00f3n.</p> <p>Tambi\u00e9n, podemos pasar los param\u00e9tros por nombre con un diccionario, donde los par\u00e1metros omitidos toman de valor por defecto <code>1</code>:</p> In\u00a0[3]: Copied! <pre>curve_fit(func, x, y, initial_params={\"w\": 3})\n</pre> curve_fit(func, x, y, initial_params={\"w\": 3}) Out[3]: <pre>Result(A=10.17 \u00b1 0.13, w=2.9951 \u00b1 0.0022)</pre>"},{"location":"ajustes/no-lineal/#funciones-no-lineales","title":"Funciones no lineales\u00b6","text":""},{"location":"ajustes/ponderado/","title":"Ajuste ponderado","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom labo1 import curve_fit\n\n\ndef lineal(x, a, b):\n    return a * x + b\n\n\nx = np.arange(10)\ny = 2 * x + 3\ny[-1] -= 20  # resta 20 al \u00faltimo valor\ny = np.random.default_rng(0).normal(y)\n\nresult = curve_fit(lineal, x, y)\nresult.plot()\nresult\n</pre> import numpy as np from labo1 import curve_fit   def lineal(x, a, b):     return a * x + b   x = np.arange(10) y = 2 * x + 3 y[-1] -= 20  # resta 20 al \u00faltimo valor y = np.random.default_rng(0).normal(y)  result = curve_fit(lineal, x, y) result.plot() result Out[1]: <pre>Result(a=0.85 \u00b1 0.68, b=6.3 \u00b1 3.7)</pre> <p>Podemos darle menor importancia a la \u00fcltima medici\u00f3n pasando un vector de incertezas o errores <code>y_err</code>:</p> In\u00a0[2]: Copied! <pre>y_err = np.ones(y.size)\ny_err[-1] = 10\n\nresult = curve_fit(lineal, x, y, y_err)\nresult.plot()\nresult\n</pre> y_err = np.ones(y.size) y_err[-1] = 10  result = curve_fit(lineal, x, y, y_err) result.plot() result Out[2]: <pre>Result(a=2.01 \u00b1 0.13, b=3.18 \u00b1 0.62)</pre> <p>De esta manera, tambi\u00e9n realiza el gr\u00e1fico con barras de errores.</p> In\u00a0[3]: Copied! <pre>curve_fit(lineal, x, y, y_err)\n</pre> curve_fit(lineal, x, y, y_err) Out[3]: <pre>Result(a=2.01 \u00b1 0.13, b=3.18 \u00b1 0.62)</pre> <p>y el mismo dividido 100:</p> In\u00a0[4]: Copied! <pre>curve_fit(lineal, x, y, y_err / 100)\n</pre> curve_fit(lineal, x, y, y_err / 100) Out[4]: <pre>Result(a=2.01 \u00b1 0.13, b=3.18 \u00b1 0.62)</pre> <p>En ambos casos, el error en los par\u00e1metros es el mismo.</p> <p>Pero, si conocemos los errores reales y no queremos que los reescale, podemos pedirselo con <code>rescale_errors=False</code>:</p> In\u00a0[5]: Copied! <pre>curve_fit(lineal, x, y, y_err / 100, rescale_errors=False)\n</pre> curve_fit(lineal, x, y, y_err / 100, rescale_errors=False) Out[5]: <pre>Result(a=2.0071 \u00b1 0.0013, b=3.1823 \u00b1 0.0061)</pre>"},{"location":"ajustes/ponderado/#ajuste-ponderado","title":"Ajuste ponderado\u00b6","text":"<p>Por defecto, <code>curve_fit</code> asume que la incerteza de las mediciones son todos iguales, y las estima a partir de los residuos. Sin embargo, es importante incluir las incertezas cuando estas no son todas iguales:</p>"},{"location":"ajustes/ponderado/#reescalado-de-errores","title":"Reescalado de errores\u00b6","text":"<p>Por defecto, <code>curve_fit</code> estima los errores a partir de los residuos del ajuste. Esto sucede incluso si le pasamos las incertezas expl\u00edcitamente.</p> <p>Por ejemplo, si el vector de errores fuese <code>y_err = [2, 4, ...]</code>, interpretar\u00eda que la segunda medici\u00f3n tiene el doble de error que la primera, pero usa una versi\u00f3n reescalada de estos para estimar los errores en los par\u00e1metros.</p> <p>Podemos ver esto al pasar el vector de errores anterior:</p>"}]}